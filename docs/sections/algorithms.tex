\section{Algoritmy}\label{sec:algorithms}

Počas vývoja výpočtovej techniky sa vyvíjali aj algoritmy pre kombinatorické hry, pričom piškvorky sú jednou z nich.
Tieto algoritmy je možné rozdeliť do 2 skupín: \emph{exaktné} a \emph{heuristické}; v tejto práci je popísaný jeden
algoritmus z každej skupiny.

\subsection{MiniMax}\label{subsec:algo-minmax}


Minimax je rozhodovacie pravidlo, podľa ktorého je možné určiť nasledujúci ťah.
Jeho princíp spočíva v maximalizovaní úžitku pre cieľového hráča a minimalizovaní úžitku pre oponenta.
Pôvodne bol vyvinutý pre \emph{hru s nulovým súčtom} (angl. zero-sum), čo je termín používaný v teórii hier.
Vyjadruje hru, kde rozhodnutia hráčov sa dajú ohodnotiť nenulovým číslom $v$, pričom súčet za sebou idúcich hier má
súčet približne 0.
Často používaná hodnota je $v=1$ alebo $v=10$.
Ťah cieľového hráča je vyjadrený kladnou hodnotou ($+v$), ťah protihráča je vyjadrený zápornou hodnotou ($-v$) a remíza
je vyjadrená hodnotou $0$.
Hru, ktorú hrajú hráči striedavo, je možné vyjadriť pomocou postupnosti pre remízu:
\begin{equation}
    v-v+v-v+ \dots = \sum{v-v} = \sum{0} = 0
\end{equation}
(z čoho vychádza aj názov \emph{hra s nulovým súčtom}), resp. pre výhru jedného z hráčov:
\begin{equation}
    \pm v+v-v+v-v+ \dots = \pm v+\sum{v-v} = \pm v+\sum{0} = \pm v
\end{equation}
Rozhodovanie vychádza práve z tohto faktu, pričom algoritmus sa snaží nájsť takú postupnosť ťahov, ktorá by bola rovná
$+v$ (resp. maximálna), čo by znamenalo výhru cieľového hráča.
Samotný rozhodovací proces sa dá vyjadriť pomocou rozhodovacieho stromu vychádzajúceho z ktoréhokoľvek stavu hry.
Stavy hry tvoria vrcholy stromu a hrany predstavujú prechody medzi týmito stavmi.
Stav hry sa dá vyjadriť pomocou $d$-rozmernej tabuľky, kde hodnoty predstavujú \textbf{X}, \textbf{O} alebo prázdne
políčko.

Nech $l$ je maximálna výška rozhodovacieho stromu.
Ak nie je explicitne zadaná, potom maximálna výška stromu je počet voľných políčok vo východiskovom stave hry.
Nech $S_{ij}$ je stav hry, kde $i$ je úroveň stromu, v ktorom sa stav hry nachádza a $j$ je poradové číslo tohto vrcholu
v rámci úrovne $i$ určené pre jednoznačnú identifikáciu stavu hry a nech $p(S_{ij})$ je v strome rodič stavu $S_{ij}$
potom $\forall i, k, j \colon p(S_{i+1,k}) = S_{ij} \colon S_{i+1,k}$ vznikne vyplnením postupne každého
prázdneho políčka znakom hráča, ktorý je na ťahu na úrovni $i$.

Pre lepšiu predstavu je možné si situáciu ukázať na príklade z \hyperref[figure:minimax-tree]{nasledujúceho obrázku}.
Koreň stromu obsahuje 3 voľné políčka, jeho synovia vzniknú vyplnením postupne každého voľného políčka znakom hráča na
úrovni 2 (tzn. \textbf{X} na pozície 1. riadok, druhý stĺpec; tretí riadok, prvý stĺpec a tretí riadok, tretí stĺpec).
Takto sa naplní celý strom a jeho listy sa ohodnotia hotnotami $v$ pre víťazný stav hry, $-v$ pre prehru a $0$ pre
remízu (na \hyperref[figure:minimax-tree]{obrázku} vyznačené čiernou farbou).

Nech
\begin{equation}
    V(S_{ij}) =
    \begin{cases}
        \min{\{V(S_{i+1,j}) \forall j\}} & \text{ak v ťahu } i \text{ je na ťahu protihráč} \\
        \max{\{V(S_{i+1,j}) \forall j\}} & \text{ak v ťahu } i \text{ je na ťahu cieľový hráč}
    \end{cases}
\end{equation}
potom $\forall S_{ij}$ je priradená hodnota $V(S_{ij})$ pre $i = 1 \dots l-1$ a $\forall j$.
Najlepší ťah je potom určený spôsobom
\begin{equation}
    S_{Best} = \max{\{V(S_{2j}) \forall j \colon p(S_{2j}) = S_{11}\}} = \max{\{V(S_{2j}) \forall j\}}
\end{equation}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/minmax-tree.png}
    \caption{Rozhodovací strom algoritmu Minimax}
\end{figure}\label{figure:minimax-tree}

% todo algorithm

Algoritmus minimax patrí medzi \emph{exaktné algoritmy}, čo znamená, že pri hľadaní nájde najlepšie (optimálne)
riešenie, no prehľadáva všetky možné riešenia (celý vyhľadávací priestor).
Vo všeobecnosti pre neprázdne hracie pole o veľkosti $r$ v $d$-rozmernom hracom priestore s $o$ obsadenými políčkami je
možné veľkosť celého vyhľadávacieho priestoru vypočítať ako
\begin{equation}
    C(r, d, o) = (r^d - o) * (r^d - o - 1) * (r^d - o - 2) \dots 1 = \prod_{i = 1}^{r^d - o}{(r^d - o - i + 1)}
\end{equation}
Pre prázdne hracie pole (teda, kde $o = 0$) platí
\begin{equation}
    C(r, d, 0) = (r^d) * (r^d - 1) * (r^d - 2) \dots 1 = \prod_{i = 1}^{r^d}{(r^d - i + 1)}
\end{equation}

Ak algoritmus vychádza z prázdneho poľa o veľkosti $3 \times 3$ ($r = 3$, $d = 2$) je veľkosť vyhľadávacieho priestoru
$\prod_{i = 1}^{9}{(10 - i)} =$ \textbf{362 880}.
Na preskúmanie takéhoto počtu je možné použiť aj komerčnú výpočtovú techniku pričom výsledok bude známy v čase menšom
ako jedna minúta, no pri veľkosti $4 \times 4$ (teda $r = 4$, $d = 2$) je $C(4, 2, 0) \approx 2*10^{13}$ a čas
prehľadania priestoru sa (za predpokladu, že preskúmanie 1 možnosti trvá 1 milisekundu) zvýši na
\emph{$\approx 663$ rokov}.
Pre trojrozmernú plochu s rozmerom $3$ ($r = 3$, $d = 3$) je $C(3, 3, 0) \approx 1.08 * 10^{28}$, čo s rovnakým časovým
predpokladom znamená, že takýto výpočet by sa skončil po $\approx 3*10^{17}$ rokoch (pre predstavu: vek vesmíru sa
odhaduje na $\approx 14$ mld. $\approx 14 * 10^9$ rokov).

\subsection{Umelá neurónová sieť}\label{subsec:algo-ann}

Z vyššie uvedeného vyplýva, že exaktné riešenie pre väčšie rozmery plochy nie je možné použiť, teda je nutné použiť
algoritmy, ktoré nenájdu najlepšie riešenie v rámci celého vyhľadávacieho priestoru (optimálne), no vedia nájsť
najlepšie riešenie v rámci okolia východiskového riešenia (suboptimálne).
Takéto algoritmy sa nazývajú \emph{heuristické} a jednou z týchto heuristických metód je \emph{umelá neurónová sieť}
angl. artificial neural network (často označovaná ako ANN).

Matematické základy pre tento algoritmus boli vytvorené už v polovici 20. storočia a sú založené na modeli
biologických neurónových sietí, z čoho vychádza aj názov a terminológia.
Sieť pozostáva z \textbf{vrstiev}, ktoré sú tvorené \textbf{neurónmi}.
Medzi jednotlivými neurónmi sú prepojenia nazývané tiež \textbf{synapsy}.
Umelú neurónovú sieť je možné reprezentovať \hyperref[figure:general-ann]{grafom}, kde vrcholy reprezentujú neuróny
a hrany synaptické spojenia.
Graf môže vyzerať nasledovne:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/general-ann.jpg}
    \caption{Všeobecná dopredná umelá neurónová sieť}
\end{figure}\label{figure:general-ann}
Nech $m$ je počet vrstiev v umelej neurónovej sieti a $n_i$ pre $i = 0 \dots m$ je počet neurónov vo vrstve $i$.
Existuje mnoho typov umelých neurónových sietí, no všeobecne sa ako ANN označuje dopredná umelá neurónová sieť.
V doprednej ANN existujú synapsy (spojenia) len medzi vrstvami $i$ a $i + 1$ pre $i = 0 \dots m - 1$, tzn. spojenia
existujú len medzi za sebou idúcimi vrstvami smerom od vstupnej ($i=0$) k výstupnej ($i=m$) vrstve.

Každému neurónu je priradená aktivačná funkcia.
Z biologického hľadiska vyjadruje akčný potenciál bunky a teda prenos informácie cez synaptické prepojenie.
V rámci umelých neurónových sietí aktivačná funkcia vyjadruje to isté: je to binárna funkcia a neurón sa aktivuje
ak na výstupe z tejto funkcie je 1.
Aktivačná funkcia závisí od vstupov predchádzajúcich neurónov.
Nech $N_{ij}$ pre $i = 1 \dots m-1$, $j = 1 \dots n_i$ je $j$-ty neurón v $i$-tej vrstve.
Na vstup do tohto neurónu sa z predchádzajúcej vrstvy prenesú informácie vo vektore $\pmb{x_{ij}}$.
$\forall h \in \pmb{x_{ij}} \colon h \in \{0, 1\}$.
Pre všetky synaptické spojenia existujú tzv. \emph{váhy}, ktoré zosilňujú (resp. zoslabujú) účinok signálu z
predchádzajúcich neurónov.
Nech $w_{ijk}$ pre $i=1 \dots m$, $k=1 \dots n_{i-1}$, $j=1 \dots n_i$ je váha pre synaptické spojenie vedúce z
neurónu $k$ vo vrstve $i-1$ k neurónu $j$ vo vrstve $i$ resp.
nech vektor $\pmb{w_{ij}}$ je vektor váh definovaných pre neurón $N_{ij}$ a nech
\begin{equation}
    z = \sum_{k=1}^{|\pmb{w_{ij}}|=|\pmb{x_{ij}}|}{w_{ijk} * x_{ijk}}-\theta
\end{equation}
kde $\theta$ je tiež označovaná ako prahová hodnota (angl. threshold).
\hyperref[figure:activation-functions]{Aktivačná funkcia} sa označuje ako $\phi(z)$.
Medzi bežne používané aktivačné funkcie patrí napríklad:
\linebreak
\textbf{Sigmoid}
\begin{equation}
    \phi(z) = \frac{1}{1+e^{-z}}
\end{equation}
\textbf{Hyperbolická funkcia}
\begin{equation}
    \phi(z) = tanh(z)
\end{equation}
\textbf{ReLu}
\begin{equation}
    \phi(z) = \max\{0, x\}
\end{equation}

Priebehy týchto funkcií sú zobrazené na grafe.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/activation-functions.png}
    \caption{Priebehy aktivačných funkcií}
\end{figure}\label{figure:activation-functions}

Pri vytváraní umelej neurónovej siete je potrebné dbať na jej správnu štruktúru a nastavenie váh.
Váhy sa dajú nastaviť tzv. učením siete.
Učenie prebieha v 2 fázach: trénovanie a testovanie siete.
V prvej fáze sa sieť naplní \emph{existujúcimi} vstupmi a výstupmi, aby vedela odhadnúť vzťah medzi nimi.
Existuje mnoho tréningových metód, no v tejto práci sú popísané 2:
\begin{itemize}
    \item algoritmus založený na delta prístupe
    \item algoritmus založený na spätnom šírení
\end{itemize}
Pri oboch prístupoch je nutné definovať tzv. trénovaciu ($T_r$) a testovaciu množinu ($T_t$).
Trénovacia množina slúži na prvotné nastavenie parametrov (váh).
Využíva sa vo fáze trénovania.
Natrénovaná sieť sa v druhej fáze otestuje pomocou testovacej množiny.

Pri \textbf{algoritme založenom na delta prístupe} (ďalej len "delta algoritmus") sa pracuje len s dvojvrstvovou
umelou neurónovou sieťou (resp. s vrstvou, ktorá ma len jednu sadu synaptických prepojení).
Princíp spočíva v postupnom vyhodnocovaní rozdielu medzi výstupom z umelej neurónovej siete pri určitom vstupe
(reálny výstup) a správnym výstupom (očakávaný výstup).
Parametre ANN sú upravované v závislosti od tohto rozdielu.
Nech $\gamma \in (0, \infty)$ je učiaci parameter, $\epsilon$ je maximálny akceptovateľný rozdiel v hodnotách reálneho
a očakávaného výstupu a $s$ je počet za sebou idúcich rozdielov výstupov (rozdiel nemusí byť vyhodnocovaný len
prostredníctvom absolútnej hodnoty, často sa využíva aj druhá mocnina rozdielu), ktoré sa nachádzali v rámci $\epsilon$.
Ďalej nech $|T_r|$ je veľkosť trénovacej množiny (teda počet vzoriek), $\mathbf{X_i}$ pre $i=1 \dots |T_r|$ je vzorka
z trénovacej množiny, ktorej je pridelený očakávaný výstup $y_i$ pre $i=1 \dots |T_r|$ a nech $y(\mathbf{X_i})$ je reálny
výstup siete ako odpoveď na vstup $\mathbf{X_i}$.
Algoritmus prebieha nasledovne:
\begin{enumerate}
    \item Inicializácia počiatočných váh $\pmb{w_1}$, prahovej hodnoty $\theta$ a parametrov $\gamma$, $\epsilon$ a $s$.
    \item pre $i = 1 \dots |T_r|$:
    \begin{enumerate}
        \item ak $(y_i-y(\mathbf{X_i}))^2>\epsilon$ \\
                $\pmb{w_{1j}}=\pmb{w_{1j}} + \gamma * (y_i-y(\mathbf{X_i})) * X_{ij}$ \\
                $\theta=\theta + \gamma * (y_i-y(\mathbf{X_i}))$
        \item ak prebehlo $s$ za sebou idúcich rozdielov výstupov, ktoré sa nachádzali v rámci $\epsilon$, tak koniec
    \end{enumerate}
\end{enumerate}
Zmena váh o $y_i-y(\mathbf{X_i})$ je celkom intuitívna, no je možné si ukázať, že to funguje aj matematicky.
Nech $\pmb{\omega_0}$ je východiskový bod pre váhy a nech $f(\pmb{x})$ je funkcia vyjadrujúca mieru chyby v sieti,
ktorú je potreba minimalizovať.
K bodu $\pmb{\omega_1}$ je možné sa z východiskového bodu posunúť v jednotkovom smere $\pmb{d}$ ($|\pmb{d}|=1$) o krok
dĺžky $\alpha$, tzn.
\begin{equation}
    \pmb{\omega_1} = \pmb{\omega_0} + \alpha\pmb{d}
\end{equation}
Tu sa naskytá otázka ako zvoliť veľkosť kroku $\alpha$, tak aby $f(\pmb{\omega_1}) < f(\pmb{\omega_0})$.
\textbf{Derivácia} funkcie vyjadruje zmenu akejkoľvek funkcie $f(x)$ v pomere s čo najmenšou zmenou parametrov tejto
funkcie, teda
\begin{equation}
    \lim_{\alpha\to0}\frac{|f(x + \alpha) - f(x)|}{\alpha}
\end{equation}
kde $\alpha$ vyjadruje zmenu parametrov.
Keďže $f(\omega_1)=f(\omega_0+\alpha\pmb{d})$ je možné tento princíp použiť na výpočet kroku $\alpha$.
\begin{equation}
    \frac{d}{d\alpha}f(\pmb{\omega_0}+\alpha\pmb{d}) = \
    \sum_{\forall i}{\frac{d f(\omega_{0_i} + \alpha{d_i})}{d \omega_{0_i} + \alpha{d_i}}}*\frac{d}{d\alpha}\omega_0+\alpha\pmb{d} = \
    \sum_{\forall i}{\frac{d f(\omega_{0_i} + \alpha{d_i})}{d \omega_{0_i} + \alpha{d_i}}}*\pmb{d} = \
    \pmb{f'}(\pmb{\omega_0}+\alpha\pmb{d})*\pmb{d}
\end{equation}
a keďže $\alpha\to0$, potom výraz je možné upraviť na
\begin{equation}
    \pmb{f'}(\pmb{\omega_0})*\pmb{d}
\end{equation}
a je to zároveň \emph{gradient} funkcie $\pmb{f}$ v bode $\pmb{\omega_0}$.
Pre skalárny súčet dvoch vektorov $\pmb{a}$ a $\pmb{b}$ platí $\pmb{a} * \pmb{b} = |\pmb{a}| * |\pmb{b}| * \cos(\beta)$, kde $\beta$
je uhol, ktorý tieto vektory zvierajú.
Keďže $\pmb{f'}(\pmb{\omega_0})$ a $\pmb{d}$ sú vektory a $|\pmb{d}|=1$ platí
\begin{equation}
    |\pmb{f'}(\pmb{\omega_0})| * \cos(\beta)
\end{equation}
Pretože $|\pmb{f'}(\pmb{\omega_0})|$ je konštanta, je potrebné vhodne zvoliť $\cos(\beta)$ tak, aby celý výraz bol čo
najmenší.
\begin{equation}
    H(\cos(\beta)) = <-1,1> \implies \cos(\beta) = -1 \iff \beta = -180\degree = -\pi
\end{equation}
Z toho vyplýva, že smer najlepšieho poklesu je opačný ku gradientu funkcie (teda $-\pmb{f'}(\pmb{\omega_0})$).
V prípade delta algoritmu je minimalizovaná funkcia rozdielu reálneho a očakávaného výstupu
\begin{equation}
    f(\pmb{\omega})=(y_i-y(\mathbf{X_i}))^2=(y_i-\phi(\sum_{\forall j}{\omega_j X_{ij}-\theta}))^2
\end{equation}
\begin{equation}
    \frac{\partial}{\partial \omega_k}f(\pmb{\omega})=2*(y_i-\phi(\sum_{\forall j}{\omega_j X_{ij}-\theta}))*(-\phi'(\sum_{\forall j}{\omega_j X_{ij}-\theta}))*X_{ik}
\end{equation}
\begin{equation}
    \frac{\partial}{\partial \omega_k}f(\pmb{\omega})=-(y_i-y(\mathbf{X_i}))*X_{ik}*(2(-\phi'(\sum_{\forall j}{\omega_j X_{ij}-\theta})))
\end{equation}
Posledný činiteľ je kladný (tzn. nezmení znamienko výrazu - derivácia kladnej funkcie) a teda smer najväčšieho poklesu
je
\begin{equation}
    \pmb{d}=(y_i-y(\mathbf{X_i}))*X_{ik}
\end{equation}
Z vyššie uvedeného vyplývajú isté vlastnosti: keďže $\phi$ je aktivačná funkcia, tak pre správne fungovanie delta
algoritmu je nutné aby bola \emph{derivovateľná}.

\textbf{Algoritmus založený na spätnom šírení} (angl. a ďalej len "backpropagation algoritmus") je zovšeobecnenie delta
algoritmu pre viacvrstvové siete.
Funguje na rovnakom princípe, ktorý sa postupne opakuje smerom od výstupnej vrstvy k vstupnej.

Navrhnutie štruktúry umelej neurónovej siete je jeden z najťažších problémov pri vytváraní riešenia problému.
Neexistuje jednoznačná a univerzálna cesta vytvorenia, ktorá by platila pre všetky problémy.
Pre jednoduché problémy je možné použiť matematické metódy pre vytvorenie umelej neurónovej siete, no pre tie väčšie
sa ANN vytvárajú skôr využitím intuície architekta siete.

\subsubsection{Umelá neurónová sieť pre hru}

Keďže váhy je možné upraviť pomocou trénovania ostáva ešte určiť štruktúru umelej neurónovej siete.
ANN by mala spĺňať tieto vlastnosti:
\begin{itemize}
    \item na vstupe má byť aktuálny stav hry
    \item na výstupe má byť informácia o tom, ktoré pole je pre aktuálneho hráča najlepšie pre ďalší krok
\end{itemize}

Stav hry je reprezentovaný vektorom $s_i$ pre $i=1 \dots r^d$, kde každých $r$ prvkov reprezentuje riadok hracej plochy,
resp. hracieho priestoru.
Napríklad pre rozmer $3 \times 3$ má vektor $s$ dĺžku 9 ($r^d=3^2$), prvky 1--3 reprezentujú prvý riadok, prvky
4--6 reprezentujú druhý riadok a prvky 7--9 reprezentujú posledný riadok.
Neurónová sieť pozostáva z 3 vrstiev (vstupná, skrytá, výstupná).
Nech $x_i$ pre $i=1 \dots 3r^d$ je neurón vo vstupnej vrstve.
Za vstup do neurónovej siete je považovaný vektor hodnôt $x_1$ až $x_{3r^d}$, kde
\begin{equation}
    x_i=
    \begin{cases}
        1 & \text{ak }s_i\text{ je prázdne a } i \in \langle 1, r^d \rangle \\
        1 & \text{ak }s_{i-r^d}\text{ je cieľový hráč a } i \in \langle r^d+1, 2r^d \rangle \\
        1 & \text{ak }s_{i-2r^d}\text{ je oponent a } i \in \langle 2r^d+1, 3r^d \rangle \\
        0 & \text{inak}
    \end{cases}
    \quad
    \text{pre }i=1 \dots 3r^d
\end{equation}
Nech $y_j$ pre $j=1 \dots r^d$ je neurón v skrytej vrstve.
Táto vrstva zabezpečuje reprezentáciu stavu hry, na základe ktorej sa nastavujú váhy pre jednotlivé hodnoty políčok
hracej plochy.
Tieto váhy je možné označiť $w_{ij}$ pre $i=1 \dots 3r^d$, $j=1 \dots r^d$, kde index $ij$ vyjadruje váhu hodnoty
$x_i$ vstupujúcu do neurónu $y_j$.
Zo vstupnej vrstvy do skrytej vrstvy sa teda prenesú hodnoty
\begin{equation}
    \sum_{i=1}^{3r^d} w_{ij}x_{ij} \quad \text{pre } j=1 \dots r^d
\end{equation}
Posledná (skrytá) vrstva má len jeden neurón $z$, ktorý vyjadruje index $i$ vo vektore $s_i$ pre najlepší možný ťah
hráča.
Váhy zo skrytej vrstvy vstupujúce do poslednej vrstvy je možné označiť $v_j$.
Zo skrytej vrstvy do výstupnej vrstvy sa prenesú hodnoty
\begin{equation}
    \sum_{j=1}^{r^d} v_{j}y_{j}
\end{equation}
Neurónová sieť vyzerá nasledovne:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{images/ann.jpg}
    \caption{Návrh neurónovej siete}
\end{figure}

